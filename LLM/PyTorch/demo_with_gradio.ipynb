{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name\n",
    "\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name\n",
    "\n",
    "\n",
    "# We instantiate the Textbox class\n",
    "textbox = gr.Textbox(label=\"Type your name here:\", placeholder=\"John Doe\", lines=2)\n",
    "\n",
    "gr.Interface(fn=greet, inputs=textbox, outputs=\"text\").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat GPT like Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(\"text-generation\")\n",
    "\n",
    "\n",
    "def predict(prompt):\n",
    "    completion = model(prompt)[0][\"generated_text\"]\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"My favorite programming language is Python. You could even say that Python is the most popular programming language, and it was created to be used as a programming language.\\n\\nBut what if you used Python as your programming language? What if you used Java, Python, or some other programming language with a different syntax and semantics?\\n\\nIt's like if you used Java on a computer, you could write a Python program as if it were Java.\\n\\nI started learning the Java language using the Java Tutorial and saw a lot more programming in this language.\\n\\nI knew that programming in Java was very easy.\\n\\nHow did you get started programming in Python?\\n\\nMy first job was teaching a course on programming in Java.\\n\\nI read a lot of Python books. I've got a computer. I started to understand Java.\\n\\nHow did you get involved in Java?\\n\\nI got interested in programming through a lot of books. I've read a lot of books on Java.\\n\\nI learned a lot of Python.\\n\\nHow did you get into Python?\\n\\nI got into programming by following the Python Tutorial.\\n\\nI got into programming because I was fascinated by the concept of data structures. I learned that there's a kind of hierarchy of\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "predict(\"My favorite programming language is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Interface class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reversing audio example\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def reverse_audio(audio):\n",
    "    sr, data = audio\n",
    "    reversed_audio = (sr, np.flipud(data))\n",
    "    return reversed_audio\n",
    "\n",
    "\n",
    "mic = gr.Audio(sources=\"microphone\", type=\"numpy\", label=\"Speak here...\")\n",
    "gr.Interface(reverse_audio, mic, \"audio\").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "notes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "\n",
    "\n",
    "def generate_tone(note, octave, duration):\n",
    "    sr = 48000\n",
    "    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)\n",
    "    frequency = a4_freq * 2 ** (tones_from_a4 / 12)\n",
    "    duration = int(duration)\n",
    "    audio = np.linspace(0, duration, duration * sr)\n",
    "    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)\n",
    "    return (sr, audio)\n",
    "\n",
    "\n",
    "gr.Interface(\n",
    "    generate_tone,\n",
    "    [\n",
    "        gr.Dropdown(notes, type=\"index\"),\n",
    "        gr.Slider(minimum=4, maximum=6, step=1),\n",
    "        gr.Number(value=1, label=\"Duration in seconds\"),\n",
    "    ],\n",
    "    \"audio\",\n",
    ").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we’ll load our speech recognition model using the pipeline() function from 🤗 Transformers. If you need a quick refresher, you can go back to that section in Chapter 1. Next, we’ll implement a transcribe_audio() function that processes the audio and returns the transcription. Finally, we’ll wrap this function in an Interface with the Audio components for the inputs and just text for the output. Altogether, the code for this application is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 22aad52 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012d844c9869493eb3dd15074008ad03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e843dea9fe4d3688fdecf05707a6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8e6e3f02b64c08a2ca31acd3d61a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b869f4cbdd4b4813b1d39deff5e8e2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a3bbeed52a4c8a8ca85816d9508ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919a748a243746b283fc346cf6df76c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "model = pipeline(\"automatic-speech-recognition\")\n",
    "\n",
    "\n",
    "def transcribe_audio(audio):\n",
    "    transcription = model(audio)[\"text\"]\n",
    "    return transcription\n",
    "\n",
    "\n",
    "gr.Interface(\n",
    "    fn=transcribe_audio,\n",
    "    inputs=gr.Audio(type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    ").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing demos with others\n",
    "\n",
    "Now that you’ve built a demo, you’ll probably want to share it with others. Gradio demos can be shared in two ways: using a temporary share link or permanent hosting on Spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "title = \"Ask Rick a Question\"\n",
    "description = \"\"\"\n",
    "The bot was trained to answer questions based on Rick and Morty dialogues. Ask Rick anything!\n",
    "<img src=\"https://huggingface.co/spaces/course-demos/Rick_and_Morty_QA/resolve/main/rick.png\" width=200px>\n",
    "\"\"\"\n",
    "\n",
    "article = \"Check out [the original Rick and Morty Bot](https://huggingface.co/spaces/kingabzpro/Rick_and_Morty_Bot) that this demo is based off of.\"\n",
    "\n",
    "gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"text\",\n",
    "    title=title,\n",
    "    description=description,\n",
    "    article=article,\n",
    "    examples=[[\"What are you doing?\"], [\"Where should we time travel to?\"]],\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not create share link. Missing file: /Users/ashkanm/.cache/huggingface/gradio/frpc/frpc_darwin_arm64_v0.3. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_darwin_arm64\n",
      "2. Rename the downloaded file to: frpc_darwin_arm64_v0.3\n",
      "3. Move the file to this location: /Users/ashkanm/.cache/huggingface/gradio/frpc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gr.Interface(classify_image, \"image\", \"label\").launch(share=True)\n",
    "# For sharing a public link: \n",
    "\n",
    "gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=\"textbox\",\n",
    "    outputs=\"text\",\n",
    "    title=title,\n",
    "    description=description,\n",
    "    article=article,\n",
    "    examples=[[\"What are you doing?\"], [\"Where should we time travel to?\"]],\n",
    ").launch(share=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hosting your demo on Hugging Face Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'demo_test'...\n",
      "remote: Enumerating objects: 4, done.\u001b[K\n",
      "remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (4/4), 1.29 KiB | 329.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "# Made a new gradio space on huggingface and \n",
    "!git clone https://huggingface.co/spaces/AshknMrd/demo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add app.y # and all the other file \n",
    "!git commit -m \"Add application file\"    \n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import gradio as gr\n",
    "from torch import nn\n",
    "\n",
    "LABELS = Path(\"demo_test/class_names.txt\").read_text().splitlines()\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, 3, padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32, 64, 3, padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(64, 128, 3, padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(1152, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, len(LABELS)),\n",
    ")\n",
    "state_dict = torch.load(\"demo_test/pytorch_model.bin\", map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def predict(im):\n",
    "    # Get RGBA image from Gradio sketchpad\n",
    "    im = im[\"composite\"]  # shape (H, W, 4)\n",
    "\n",
    "    # Ensure it's a NumPy array\n",
    "    if not isinstance(im, np.ndarray):\n",
    "        im = np.array(im)\n",
    "\n",
    "    # Convert RGBA to grayscale\n",
    "    im = im[:, :, :3]  # Drop alpha\n",
    "    im = np.dot(im, [0.299, 0.587, 0.114])  # Convert to grayscale (H, W)\n",
    "\n",
    "    # Resize to 28x28 (what your model expects)\n",
    "    from PIL import Image\n",
    "    im = Image.fromarray(im.astype(np.uint8)).resize((28, 28))\n",
    "    im = np.array(im)\n",
    "\n",
    "    # Normalize and convert to tensor\n",
    "    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0  # shape [1, 1, 28, 28]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
    "    values, indices = torch.topk(probabilities, 5)\n",
    "\n",
    "    return {LABELS[i]: v.item() for i, v in zip(indices, values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = gr.Interface(\n",
    "    predict,\n",
    "    inputs=\"sketchpad\",\n",
    "    outputs=\"label\",\n",
    "    theme=\"huggingface\",\n",
    "    title=\"Sketch Recognition\",\n",
    "    description=\"Who wants to play Pictionary? Draw a common object like a shovel or a laptop, and the algorithm will guess in real time!\",\n",
    "    article=\"<p style='text-align: center'>Sketch Recognition | Demo Model</p>\",\n",
    "    live=True,\n",
    ")\n",
    "# interface.launch(share=True)\n",
    "interface.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrations with the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching model from: https://huggingface.co/EleutherAI/gpt-j-6b\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/external.py\", line 472, in query_huggingface_inference_endpoints\n",
      "    data = fn(*data)\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/external_utils.py\", line 129, in text_generation_inner\n",
      "    return input + client.text_generation(input)\n",
      "                   ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/huggingface_hub/inference/_client.py\", line 2296, in text_generation\n",
      "    provider_helper = get_provider_helper(self.provider, task=\"text-generation\", model=model_id)\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/huggingface_hub/inference/_providers/__init__.py\", line 191, in get_provider_helper\n",
      "    provider = next(iter(provider_mapping)).provider\n",
      "               ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "StopIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/blocks.py\", line 2235, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/external.py\", line 474, in query_huggingface_inference_endpoints\n",
      "    external_utils.handle_hf_error(e)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/external_utils.py\", line 292, in handle_hf_error\n",
      "    raise Error(str(e)) from e\n",
      "gradio.exceptions.Error: ''\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "title = \"GPT-J-6B\"\n",
    "description = \"Gradio Demo for GPT-J 6B, a transformer model trained using Ben Wang's Mesh Transformer JAX. 'GPT-J' refers to the class of model, while '6B' represents the number of trainable parameters. To use it, simply add your text, or click one of the examples to load them. Read more at the links below.\"\n",
    "article = \"<p style='text-align: center'><a href='https://github.com/kingoflolz/mesh-transformer-jax' target='_blank'>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</a></p>\"\n",
    "\n",
    "gr.load(\n",
    "    \"huggingface/EleutherAI/gpt-j-6b\",\n",
    "    inputs=gr.Textbox(lines=5, label=\"Input Text\"),\n",
    "    title=title,\n",
    "    description=description,\n",
    "    article=article,\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface.load(\"spaces/abidlabs/remove-bg\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EventListener._setup.<locals>.event_trigger() got an unexpected keyword argument 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspaces/abidlabs/remove-bg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwebcam\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRemove your webcam background!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m.launch()\n",
      "\u001b[31mTypeError\u001b[39m: EventListener._setup.<locals>.event_trigger() got an unexpected keyword argument 'title'"
     ]
    }
   ],
   "source": [
    "gr.Interface.load(\n",
    "    \"spaces/abidlabs/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\"\n",
    ").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Interface features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/components/base.py:433: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  obj = utils.component_or_layout_class(cls_name)(render=render)\n",
      "/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/interface.py:425: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def chat(message, history):\n",
    "    history = history or []\n",
    "    if message.startswith(\"How many\"):\n",
    "        response = random.randint(1, 10)\n",
    "    elif message.startswith(\"How\"):\n",
    "        response = random.choice([\"Great\", \"Good\", \"Okay\", \"Bad\"])\n",
    "    elif message.startswith(\"Where\"):\n",
    "        response = random.choice([\"Here\", \"There\", \"Somewhere\"])\n",
    "    else:\n",
    "        response = \"I don't know\"\n",
    "    history.append((message, response))\n",
    "    return history, history\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    chat,\n",
    "    [\"text\", \"state\"],\n",
    "    [\"chatbot\", \"state\"],\n",
    "    # allow_screenshot=False,\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgr\u001b[39;00m\n\u001b[32m      6\u001b[39m inception_net = tf.keras.applications.MobileNetV2()  \u001b[38;5;66;03m# load the model\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "inception_net = tf.keras.applications.MobileNetV2()  # load the model\n",
    "\n",
    "# Download human-readable labels for ImageNet.\n",
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "labels = response.text.split(\"\\n\")\n",
    "\n",
    "\n",
    "def classify_image(inp):\n",
    "    inp = inp.reshape((-1, 224, 224, 3))\n",
    "    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n",
    "    prediction = inception_net.predict(inp).flatten()\n",
    "    return {labels[i]: float(prediction[i]) for i in range(1000)}\n",
    "\n",
    "\n",
    "image = gr.Image(shape=(224, 224))\n",
    "label = gr.Label(num_top_classes=3)\n",
    "\n",
    "title = \"Gradio Image Classifiction + Interpretation Example\"\n",
    "gr.Interface(\n",
    "    fn=classify_image, inputs=image, outputs=label, interpretation=\"default\", title=title\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "# Load pretrained MobileNetV2 model\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load ImageNet labels\n",
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "labels = response.text.strip().split(\"\\n\")\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "def classify_image(img):\n",
    "    img = transform(img).unsqueeze(0)  # [1, 3, 224, 224]\n",
    "    with torch.no_grad():\n",
    "        preds = model(img).squeeze()\n",
    "        probs = torch.nn.functional.softmax(preds, dim=0)\n",
    "    topk = torch.topk(probs, 3)\n",
    "    return {labels[i]: float(probs[i]) for i in topk.indices}\n",
    "\n",
    "image = gr.Image(type=\"pil\")\n",
    "label = gr.Label(num_top_classes=3)\n",
    "title = \"Gradio Image Classification Example (PyTorch)\"\n",
    "\n",
    "gr.Interface(\n",
    "    fn=classify_image,\n",
    "    inputs=image,\n",
    "    outputs=label,\n",
    "    title=title\n",
    ").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Gradio Blocks\n",
    "\n",
    "Now, what’s the difference between Interface and Blocks?\n",
    "\n",
    "⚡ Interface: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.\n",
    "\n",
    "🧱 Blocks: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using Blocks (as in “building blocks”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def flip_text(x):\n",
    "    return x[::-1]\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    # Flip Text!\n",
    "    Start typing below to see the output.\n",
    "    \"\"\"\n",
    "    )\n",
    "    input = gr.Textbox(placeholder=\"Flip this text\")\n",
    "    output = gr.Textbox()\n",
    "\n",
    "    input.change(fn=flip_text, inputs=input, outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "\n",
    "def flip_text(x):\n",
    "    return x[::-1]\n",
    "\n",
    "\n",
    "def flip_image(x):\n",
    "    return np.fliplr(x)\n",
    "\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(\"Flip text or image files using this demo.\")\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Flip Text\"):\n",
    "            with gr.Row():\n",
    "                text_input = gr.Textbox()\n",
    "                text_output = gr.Textbox()\n",
    "            text_button = gr.Button(\"Flip\")\n",
    "        with gr.TabItem(\"Flip Image\"):\n",
    "            with gr.Row():\n",
    "                image_input = gr.Image()\n",
    "                image_output = gr.Image()\n",
    "            image_button = gr.Button(\"Flip\")\n",
    "\n",
    "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
    "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching model from: https://huggingface.co/EleutherAI/gpt-j-6B\n",
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/external.py\", line 472, in query_huggingface_inference_endpoints\n",
      "    data = fn(*data)\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/external_utils.py\", line 129, in text_generation_inner\n",
      "    return input + client.text_generation(input)\n",
      "                   ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/huggingface_hub/inference/_client.py\", line 2296, in text_generation\n",
      "    provider_helper = get_provider_helper(self.provider, task=\"text-generation\", model=model_id)\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/huggingface_hub/inference/_providers/__init__.py\", line 191, in get_provider_helper\n",
      "    provider = next(iter(provider_mapping)).provider\n",
      "               ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "StopIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/blocks.py\", line 2235, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/y_/1pxsz__d753g2nqbbjh_gzh00000gn/T/ipykernel_77170/2459505582.py\", line 8, in complete_with_gpt\n",
      "    return text[:-50] + api(text[-50:])\n",
      "                        ~~~^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/blocks.py\", line 1667, in __call__\n",
      "    outputs = client_utils.synchronize_async(\n",
      "        self.process_api,\n",
      "    ...<4 lines>...\n",
      "        explicit_call=True,\n",
      "    )\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio_client/utils.py\", line 924, in synchronize_async\n",
      "    return fsspec.asyn.sync(fsspec.asyn.get_loop(), func, *args, **kwargs)  # type: ignore\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/fsspec/asyn.py\", line 103, in sync\n",
      "    raise return_result\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/fsspec/asyn.py\", line 56, in _runner\n",
      "    result[0] = await coro\n",
      "                ^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/blocks.py\", line 2235, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/external.py\", line 474, in query_huggingface_inference_endpoints\n",
      "    external_utils.handle_hf_error(e)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/opt/anaconda3/envs/temp/lib/python3.13/site-packages/gradio/external_utils.py\", line 292, in handle_hf_error\n",
      "    raise Error(str(e)) from e\n",
      "gradio.exceptions.Error: ''\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "api = gr.load(\"huggingface/EleutherAI/gpt-j-6B\")\n",
    "\n",
    "\n",
    "def complete_with_gpt(text):\n",
    "    # Use the last 50 characters of the text as context\n",
    "    return text[:-50] + api(text[-50:])\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    textbox = gr.Textbox(placeholder=\"Type here and press enter...\", lines=4)\n",
    "    btn = gr.Button(\"Generate\")\n",
    "\n",
    "    btn.click(complete_with_gpt, textbox, textbox)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multi-step demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "\n",
    "def speech_to_text(speech):\n",
    "    text = asr(speech)[\"text\"]\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    return classifier(text)[0][\"label\"]\n",
    "\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    audio_file = gr.Audio(type=\"filepath\")\n",
    "    text = gr.Textbox()\n",
    "    label = gr.Label()\n",
    "\n",
    "    b1 = gr.Button(\"Recognize Speech\")\n",
    "    b2 = gr.Button(\"Classify Sentiment\")\n",
    "\n",
    "    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n",
    "    b2.click(text_to_sentiment, inputs=text, outputs=label)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Component Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def change_textbox(choice):\n",
    "    if choice == \"short\":\n",
    "        return gr.Textbox.update(lines=2, visible=True)\n",
    "    elif choice == \"long\":\n",
    "        return gr.Textbox.update(lines=8, visible=True)\n",
    "    else:\n",
    "        return gr.Textbox.update(visible=False)\n",
    "\n",
    "\n",
    "with gr.Blocks() as block:\n",
    "    radio = gr.Radio(\n",
    "        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n",
    "    )\n",
    "    text = gr.Textbox(lines=2, interactive=True)\n",
    "\n",
    "    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n",
    "    block.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
